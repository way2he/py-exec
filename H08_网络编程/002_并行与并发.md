# Python并行与并发详解

## 一、基本概念

### 1.1 并行（Parallelism）
并行指**同一时刻多个任务在多个CPU核心上同时执行**，本质是利用多核CPU的物理资源实现真正的同时运行。典型场景：视频渲染、大数据计算等CPU密集型任务。

### 1.2 并发（Concurrency）
并发指**在一段时间内通过任务切换处理多个任务**，但同一时刻可能只有一个任务运行（单核场景）。核心是通过调度算法（如时间片轮转）提升资源利用率。典型场景：Web服务器处理多请求、I/O等待时的任务切换。

### 1.3 关键区别
| 特性        | 并行                  | 并发                  |
|-------------|-----------------------|-----------------------|
| 物理基础    | 多核CPU              | 单/多核均可           |
| 执行时机    | 同时运行              | 交替运行              |
| 适用场景    | CPU密集型任务         | I/O密集型任务         |

## 二、核心模块概览
Python提供了三大核心模块实现并行与并发：
- `threading`：基于线程的并发（适用于I/O密集型）
- `multiprocessing`：基于进程的并行（适用于CPU密集型）
- `asyncio`：基于协程的异步并发（适用于高并发I/O场景）

## 三、`threading`模块详解

### 3.1 线程创建方式
Python通过`threading.Thread`类实现线程创建，支持两种核心方式：

#### 方式1：继承`Thread`类（重写`run`方法）
```python
import threading
import time

class MyThread(threading.Thread):
    def __init__(self, name: str):
        super().__init__(name=name)  # 调用父类初始化

    def run(self) -> None:
        # 线程核心逻辑
        for i in range(3):
            print(f"{self.name} 执行第{i+1}次循环（继承方式）")
            time.sleep(0.5)

if __name__ == "__main__":
    thread1 = MyThread("线程A")
    thread2 = MyThread("线程B")
    thread1.start()  # 启动线程（自动调用run方法）
    thread2.start()
    thread1.join()   # 等待线程执行完毕
    thread2.join()
    print("所有线程执行完成")
```

#### 方式2：通过`target`参数指定函数
```python
import threading
import time

def thread_task(name: str) -> None:
    for i in range(3):
        print(f"{name} 执行第{i+1}次循环（target方式）")
        time.sleep(0.5)

if __name__ == "__main__":
    thread1 = threading.Thread(target=thread_task, args=("线程C",))
    thread2 = threading.Thread(target=thread_task, args=("线程D",))
    thread1.start()
    thread2.start()
    thread1.join()
    thread2.join()
    print("所有线程执行完成")
```

### 3.2 线程同步机制
由于Python的GIL（全局解释器锁）限制，多线程在CPU密集型任务中无法真正并行，但I/O密集型任务（如文件读写、网络请求）仍需处理线程安全问题。常用同步工具：

#### 3.2.1 `threading.Lock`（互斥锁）
```python
import threading

counter = 0
lock = threading.Lock()  # 创建锁对象

def increment():
    global counter
    for _ in range(100000):
        with lock:  # 自动获取/释放锁
            counter += 1

if __name__ == "__main__":
    threads = [threading.Thread(target=increment) for _ in range(10)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()
    print(f"最终counter值：{counter}")  # 正确输出1000000
```

#### 3.2.2 `threading.Event`（事件）
用于线程间通信，通过`set()`/`clear()`控制其他线程的等待状态：
```python
import threading
import time

event = threading.Event()

def waiter():
    print("等待事件触发...")
    event.wait()  # 阻塞直到event被set
    print("事件已触发！")

def setter():
    time.sleep(2)
    event.set()  # 触发事件

if __name__ == "__main__":
    threading.Thread(target=waiter).start()
    threading.Thread(target=setter).start()
```

### 3.3 适用场景与陷阱
- **适用场景**：I/O密集型任务（如Web爬虫、文件批量读写），因线程切换成本远低于进程。
- **常见陷阱**：
  - GIL限制：CPU密集型任务中多线程无法提升性能（本质仍是单线程执行）。
  - 死锁：多个线程循环等待对方释放锁（需遵循“按序加锁”原则）。
  - 资源竞争：未加锁时操作非原子性变量（如`counter += 1`实际包含读取、修改、写入三步）。

## 四、`multiprocessing`模块详解

### 4.1 进程核心特性
进程是操作系统资源分配的基本单位，每个进程拥有独立的内存空间（解决GIL限制）。Python通过`multiprocessing`模块实现跨平台进程管理，核心优势：
- 真正并行：绕过GIL限制，充分利用多核CPU
- 隔离性：进程间内存独立，崩溃不会影响其他进程
- 兼容性：支持Windows/Linux/macOS

### 4.2 进程创建与基础操作
#### 4.2.1 进程创建方式
与`threading`类似，支持继承`Process`类或通过`target`参数指定函数：

```python
import multiprocessing
import time

# 方式1：继承Process类
class MyProcess(multiprocessing.Process):
    def __init__(self, name: str):
        super().__init__(name=name)

    def run(self) -> None:
        for i in range(3):
            print(f"{self.name} 执行第{i+1}次循环（继承方式）")
            time.sleep(0.5)

# 方式2：通过target指定函数
def process_task(name: str) -> None:
    for i in range(3):
        print(f"{name} 执行第{i+1}次循环（target方式）")
        time.sleep(0.5)

if __name__ == "__main__":
    # 创建并启动进程
    p1 = MyProcess("进程A")
    p2 = multiprocessing.Process(target=process_task, args=("进程B",))
    p1.start()
    p2.start()
    p1.join()  # 等待进程执行完毕
    p2.join()
    print("所有进程执行完成")
```

#### 4.2.2 进程间通信（IPC）
由于进程内存隔离，需通过`Queue`（队列）、`Pipe`（管道）等机制实现数据传递：

##### 4.2.2.1 `multiprocessing.Queue`（线程安全队列）
```python
import multiprocessing

def producer(queue: multiprocessing.Queue):
    for i in range(3):
        queue.put(f"数据{i}")  # 向队列写入数据
        print(f"生产者发送：数据{i}")

def consumer(queue: multiprocessing.Queue):
    while True:
        data = queue.get()  # 从队列读取数据（阻塞直到有数据）
        if data is None:
            break
        print(f"消费者接收：{data}")

if __name__ == "__main__":
    queue = multiprocessing.Queue()
    p_producer = multiprocessing.Process(target=producer, args=(queue,))
    p_consumer = multiprocessing.Process(target=consumer, args=(queue,))
    p_producer.start()
    p_consumer.start()
    p_producer.join()
    queue.put(None)  # 发送终止信号
    p_consumer.join()
```

##### 4.2.2.2 `multiprocessing.Pipe`（双向管道）
```python
import multiprocessing

def sender(conn):
    conn.send("你好，接收端！")  # 发送消息
    print("发送端：消息已发送")
    conn.close()

def receiver(conn):
    msg = conn.recv()  # 接收消息（阻塞直到收到）
    print(f"接收端：收到消息 '{msg}'")
    conn.close()

if __name__ == "__main__":
    parent_conn, child_conn = multiprocessing.Pipe()
    p_sender = multiprocessing.Process(target=sender, args=(parent_conn,))
    p_receiver = multiprocessing.Process(target=receiver, args=(child_conn,))
    p_sender.start()
    p_receiver.start()
    p_sender.join()
    p_receiver.join()
```

### 4.3 进程池（Pool）的高效使用
对于批量任务（如100个独立计算任务），使用`Pool`可自动管理进程生命周期，减少创建/销毁开销：

```python
import multiprocessing
import time

def task(num: int) -> int:
    time.sleep(0.1)
    return num * 2

if __name__ == "__main__":
    # 创建包含4个进程的进程池（根据CPU核心数调整）
    with multiprocessing.Pool(processes=4) as pool:
        # 方式1：map（阻塞直到所有任务完成）
        results = pool.map(task, range(10))
        print("map结果：", results)

        # 方式2：apply_async（异步执行，支持回调）
        def callback(res):
            print(f"异步任务完成，结果：{res}")

        for i in range(10):
            pool.apply_async(task, args=(i,), callback=callback)
        pool.close()  # 关闭进程池（不再接受新任务）
        pool.join()  # 等待所有任务完成
```

### 4.4 适用场景与最佳实践
- **适用场景**：
  - CPU密集型任务（如矩阵运算、图像渲染、机器学习训练）
  - 需要隔离错误的场景（如第三方库可能崩溃）
- **最佳实践**：
  - 进程数限制：根据CPU核心数设置（通常`multiprocessing.cpu_count()`），避免进程过多导致上下文切换开销
  - 数据序列化：通过`Queue`/`Pipe`传递的数据需可序列化（推荐使用JSON或`pickle`）
  - 资源释放：及时关闭`Queue`/`Pipe`连接，避免资源泄漏

### 4.5 常见陷阱与规避
- **陷阱1**：全局变量不同步
  进程拥有独立内存空间，修改全局变量不会影响其他进程。
  **规避**：通过`Queue`/`Manager`共享状态
- **陷阱2**：Windows下的`if __name__ == "__main__"`要求
  Windows进程创建需要主模块保护，否则会递归创建进程导致崩溃。
  **规避**：所有进程启动代码必须放在`if __name__ == "__main__"`块内
- **陷阱3**：资源竞争（如文件写入）
  多个进程同时写入同一文件会导致数据混乱。
  **规避**：使用`multiprocessing.Lock`或集中式写入进程

## 五、`asyncio`异步编程详解

### 5.1 核心概念体系
`asyncio`是Python标准库中基于协程（Coroutine）的异步I/O框架，通过单线程+事件循环实现高效并发。核心术语：
- **协程（Coroutine）**：通过`async def`定义的可暂停/恢复的函数（本质是生成器）
- **事件循环（Event Loop）**：异步任务的调度中心（单线程执行，管理所有协程的挂起/恢复）
- **任务（Task）**：协程的包装对象，用于在事件循环中调度
- **Future**：表示尚未完成的异步操作结果

### 5.2 基础使用流程
#### 5.2.1 协程函数定义与执行
```python
import asyncio

# 定义异步函数（协程）
async def async_task(name: str, delay: float) -> None:
    print(f"{name} 开始执行（协程创建）")
    await asyncio.sleep(delay)  # 模拟异步I/O操作（主动让出控制权）
    print(f"{name} 执行完成（耗时{delay}s）")

# 事件循环执行入口
async def main():
    # 创建任务（自动加入事件循环）
    task1 = asyncio.create_task(async_task("任务A", 1.0))
    task2 = asyncio.create_task(async_task("任务B", 0.5))

    # 等待所有任务完成（相当于await task1; await task2）
    await asyncio.gather(task1, task2)

if __name__ == "__main__":
    asyncio.run(main())  # 启动事件循环
```
**执行输出**（时间线）：
```
任务A 开始执行（协程创建）
任务B 开始执行（协程创建）
任务B 执行完成（耗时0.5s）  # 0.5s时B完成
任务A 执行完成（耗时1.0s）  # 1.0s时A完成
```

#### 5.2.2 异步I/O实战：HTTP请求
使用`aiohttp`（异步HTTP客户端库）实现并发请求：
```python
import asyncio
from aiohttp import ClientSession

async def fetch_url(session: ClientSession, url: str) -> int:
    async with session.get(url) as response:
        return response.status  # 获取HTTP状态码

async def main():
    urls = ["https://www.baidu.com", "https://www.taobao.com", "https://www.jd.com"]
    async with ClientSession() as session:
        # 创建任务列表
        tasks = [fetch_url(session, url) for url in urls]
        # 并发执行所有任务
        results = await asyncio.gather(*tasks)
        for url, status in zip(urls, results):
            print(f"{url} → HTTP状态码：{status}")

if __name__ == "__main__":
    asyncio.run(main())
```

### 5.3 高级特性与最佳实践
#### 5.3.1 异步上下文管理器
用于管理异步资源（如数据库连接、文件句柄），通过`async with`实现自动获取/释放：
```python
import asyncio

class AsyncResource:
    async def __aenter__(self):
        print("异步资源获取")
        return self  # 返回资源对象

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        print("异步资源释放")

async def main():
    async with AsyncResource():
        print("使用资源执行操作")

asyncio.run(main())
```

#### 5.3.2 任务取消与超时控制
通过`asyncio.wait_for`设置超时，避免任务无限阻塞：
```python
import asyncio

async def slow_task():
    await asyncio.sleep(3)
    return "完成"

async def main():
    try:
        # 等待任务2秒，超时则抛出异常
        result = await asyncio.wait_for(slow_task(), timeout=2.0)
        print(f"任务结果：{result}")
    except asyncio.TimeoutError:
        print("任务超时！")

asyncio.run(main())  # 输出：任务超时！
```

### 5.4 适用场景与性能优势
- **适用场景**：
  - 高并发I/O场景（如Web服务器、API网关、实时数据采集）
  - 单线程多任务调度（避免多线程/进程的上下文切换开销）
- **性能对比**：
  | 方案         | 并发量（单线程） | 内存占用 | 适用任务类型       |
  |--------------|------------------|----------|--------------------|
  | `threading`  | 100~1000         | 较高     | I/O密集型（轻量）  |
  | `multiprocessing` | 10~100         | 高       | CPU密集型          |
  | `asyncio`    | 10000+          | 极低     | I/O密集型（高并发） |

### 5.5 常见陷阱与规避
- **陷阱1**：在异步代码中使用同步I/O
  如用`time.sleep()`代替`asyncio.sleep()`，会导致事件循环阻塞。
  **规避**：所有I/O操作必须使用异步版本（如`aiohttp`代替`requests`）。
- **陷阱2**：忘记`await`关键字
  未`await`的协程不会实际执行，导致逻辑错误。
  **规避**：使用`asyncio.create_task()`或显式`await`确保协程被调度。
- **陷阱3**：事件循环未正确关闭
  长时间运行的应用可能泄漏资源。
  **规避**：在`main`函数中使用`asyncio.run()`（自动关闭循环）或手动调用`loop.close()`。

## 六、经验总结与综合建议
### 6.1 方案选择决策树
1. **CPU密集型任务**：优先`multiprocessing`（绕过GIL，利用多核）
2. **I/O密集型任务**：
   - 轻量并发（<1000）：`threading`（代码简单）
   - 高并发（>1000）：`asyncio`（性能最优）
3. **混合任务**：`multiprocessing`+`asyncio`（进程池+协程处理子任务）

### 6.2 性能优化要点
- 减少上下文切换：`asyncio`的协程切换成本是微秒级（远低于线程的毫秒级）
- 资源复用：使用连接池（如`aiohttp.ClientSession`）避免重复建立连接
- 批量处理：通过`asyncio.gather()`批量提交任务，减少调度开销

### 6.3 未来趋势
Python 3.11+引入`asyncio.TaskGroup`（结构化并发）和更快的`asyncio`实现（`uvloop`可选），进一步简化异步代码编写并提升性能，建议升级到最新版本（≥3.11）。

（全文完）